{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import os\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "    elif ext == '.npy':\n",
    "        if not isinstance(obj, np.ndarray):\n",
    "            obj = np.array(obj)\n",
    "        np.save(filename, obj)\n",
    "    else:\n",
    "        pass # raise Error\n",
    "\n",
    "def load_pickle(filename):\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    if ext in ['.pkl','.p','.data']:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    elif ext == '.npy':\n",
    "        return np.load(filename)\n",
    "    else:\n",
    "        return None # raise Error\n",
    "    \n",
    "def sort_dict(m: dict) -> dict:\n",
    "    return dict(sorted(m.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# graph_d = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/topic_diffusion_graph_full_windowsize7.data\")\n",
    "# for k, graph in graph_d.items():\n",
    "#     edges_t = graph.edge_index\n",
    "#     edges = list(zip(edges_t[0].tolist(), edges_t[1].tolist()))\n",
    "#     # print(edges[0])\n",
    "#     # print(edges[-10:])\n",
    "#     for i in range()\n",
    "#     print((4001,4001) in edges)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate Generation_1\n",
    "# import json\n",
    "\n",
    "# sample_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/input_sample.jsonl\"\n",
    "# samples = []\n",
    "# with open(sample_filepath, 'r') as f:\n",
    "#     for line in f:\n",
    "#         samples.append(json.loads(line))\n",
    "\n",
    "# result_filepath = \"/root/pyHeter-GAT/preprocess-llm/topicGPT/script/generation_1_tmp copy.jsonl\"\n",
    "# results = []\n",
    "# with open(result_filepath, 'r') as f:\n",
    "#     for line in f:\n",
    "#         results.append(json.loads(line))\n",
    "\n",
    "# save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1/generation_1.jsonl\"\n",
    "# with open(save_filepath, 'w') as f:\n",
    "#     for idx, sample in enumerate(samples):\n",
    "#         result = {\n",
    "#             **sample,\n",
    "#             \"responses\": results[idx][\"response\"],\n",
    "#         }\n",
    "#         f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9388\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# sample_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm1/input_sample.jsonl\"\n",
    "# samples = []\n",
    "# with open(sample_filepath, 'r') as f:\n",
    "#     for line in f:\n",
    "#         samples.append(json.loads(line))\n",
    "\n",
    "# result_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm1/generation_1/generation_1.jsonl\"\n",
    "# results = []\n",
    "# with open(result_filepath, 'r') as f:\n",
    "#     for line in f:\n",
    "#         results.append(json.loads(line))\n",
    "\n",
    "# save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm1/generation_1/generation_1.jsonl\"\n",
    "# with open(save_filepath, 'w') as f:\n",
    "#     for idx, sample in enumerate(samples):\n",
    "#         if idx == 1000: continue\n",
    "#         i = idx\n",
    "#         if i > 1000: i -= 1\n",
    "#         try:\n",
    "#             result = {\n",
    "#                 **sample,\n",
    "#                 \"responses\": results[i][\"responses\"],\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             print(i)\n",
    "#             print(e)\n",
    "#         f.write(json.dumps(result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_&_social_concern 43511.8 25\n",
      "arts_&_culture 24074 14\n",
      "diaries_&_daily_life 16859 10\n",
      "science_&_technology 16049 9\n",
      "business_&_entrepreneurs 12969 7\n",
      "film_tv_&_video 11124 6\n",
      "food_&_dining 10025 6\n",
      "sports 7190 4\n",
      "other_hobbies 6597 4\n",
      "celebrity_&_pop_culture 6398 4\n",
      "travel_&_adventure 5748 3\n",
      "learning_&_educational 4090 2\n",
      "music 3170 2\n",
      "fitness_&_health 3134 2\n",
      "family 2253 1\n",
      "gaming 1557 1\n",
      "relationships 1033 1\n",
      "fashion_&_style 768 0\n",
      "youth_&_student_life 100 0\n"
     ]
    }
   ],
   "source": [
    "# # 1. 观察Bertopic生成的话题标签数量级\n",
    "\n",
    "# filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/cascades.data\"\n",
    "# cascade_dict = load_pickle(filepath)\n",
    "\n",
    "# cnt = {}\n",
    "\n",
    "# label_mp = {\n",
    "#     0: \"arts_&_culture\",\n",
    "#     1: \"business_&_entrepreneurs\",\n",
    "#     2: \"celebrity_&_pop_culture\",\n",
    "#     3: \"diaries_&_daily_life\",\n",
    "#     4: \"family\",\n",
    "#     5: \"fashion_&_style\",\n",
    "#     6: \"film_tv_&_video\",\n",
    "#     7: \"fitness_&_health\",\n",
    "#     8: \"food_&_dining\",\n",
    "#     9: \"gaming\",\n",
    "#     10: \"learning_&_educational\",\n",
    "#     11: \"music\",\n",
    "#     12: \"news_&_social_concern\",\n",
    "#     13: \"other_hobbies\",\n",
    "#     14: \"relationships\",\n",
    "#     15: \"science_&_technology\",\n",
    "#     16: \"sports\",\n",
    "#     17: \"travel_&_adventure\",\n",
    "#     18: \"youth_&_student_life\"\n",
    "# }\n",
    "\n",
    "# for key, cascades in cascade_dict.items():\n",
    "#     for b_label in cascades[\"label\"]:\n",
    "#         if b_label not in cnt: cnt[b_label] = 0\n",
    "#         cnt[b_label] += 1\n",
    "\n",
    "# # print(cnt)\n",
    "# cnt[12] /= 10\n",
    "# tot = sum([v for k, v in cnt.items()])\n",
    "# for k, v in sort_dict(cnt).items():\n",
    "#     print(label_mp[k], v, round(100*v/tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_&_social_concern 399217.6 31\n",
      "food_&_dining 304791 24\n",
      "arts_&_culture 223379 18\n",
      "science_&_technology 71271 6\n",
      "diaries_&_daily_life 31752 3\n",
      "other_hobbies 29770 2\n",
      "celebrity_&_pop_culture 27953 2\n",
      "business_&_entrepreneurs 27397 2\n",
      "film_tv_&_video 20163 2\n",
      "fitness_&_health 18130 1\n",
      "sports 15728 1\n",
      "travel_&_adventure 15472 1\n",
      "family 13766 1\n",
      "gaming 13201 1\n",
      "learning_&_educational 12768 1\n",
      "music 12510 1\n",
      "relationships 10685 1\n",
      "fashion_&_style 10504 1\n",
      "youth_&_student_life 10286 1\n"
     ]
    }
   ],
   "source": [
    "# # 2. 统计最后生成的话题通道增广图的边数量级\n",
    "# # 结论: 1)topic=12的图的边数量级在400w, topic=8/0的数量级在30w, 其余话题基本在几w的量级(最少的1w+)\n",
    "# # 2) 从用户话题推文到话题通道的扩展量级维持在10左右, 即推文数量级*10～话题通道的数量级\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "# hedge_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_graph/topic_diffusion_graph_full_windowsize300.data\"\n",
    "# hedge_graph_dict = load_pickle(hedge_filepath)\n",
    "\n",
    "# # 去掉原始的传播边\n",
    "# graph_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/edges.data\"\n",
    "# graph = load_pickle(graph_filepath)\n",
    "# n_org_edges = len(graph)\n",
    "\n",
    "# cnt_after = {}\n",
    "# for key, hgraph in hedge_graph_dict.items():\n",
    "#     # print(hgraph.edge_index)\n",
    "#     _, n_edges = hgraph.edge_index.shape\n",
    "#     # print(n_edges)\n",
    "#     cnt_after[key] = n_edges - n_org_edges\n",
    "#     # print(type(hgraph))\n",
    "\n",
    "# cnt_after[12] /= 10\n",
    "# tot2 = sum([v for k, v in cnt_after.items()])\n",
    "# for k,v in sort_dict(cnt_after).items():\n",
    "#     # print(k, v, cnt[k], v / cnt[k])\n",
    "#     print(label_mp[k], v, round(100*v/tot2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/virtualenvs/pyHeter-GAT-_-uOEIOh/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/generation_1/generation_1_ex_2000user.jsonl\n",
      "146822\n"
     ]
    }
   ],
   "source": [
    "# 回想一下之前怎么构造的图\n",
    "\n",
    "# 参考utils/graph.py中的build_heteredge_mats函数\n",
    "# 1. 按照级联粒度，使用滑动窗口来构造话题通道\n",
    "# P.S. 降采样了12: news_&_social_concern话题，因为它在总推文中的占比约为43w/52w\n",
    "# 2. 按照话题粒度合并不同级联内的话题通道\n",
    "# 3. 将每个话题的图和原始拓扑图合并\n",
    "# Option: 4. 采用Motif方式做数据增强\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import regex\n",
    "import random\n",
    "from utils.log import logger\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 1. Prepare input from TopicGPT\n",
    "# P.S. start from 2000 seed users (out of 10000 total users)\n",
    "\n",
    "# 1-2. Prepare topic to interest mappings\n",
    "\n",
    "# blob_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/generation_1/generation_1_bertopic_ex_same_user.jsonl\"\n",
    "# gt_ft_mapping = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/gt_ft_mapping.data\")\n",
    "\n",
    "# blob_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/generation_1/generation_1_ex_same_user.jsonl\"\n",
    "blob_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/generation_1/generation_1_ex_2000user.jsonl\"\n",
    "gt_ft_mapping = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/gt_ft_mapping.data\")\n",
    "\n",
    "results = []\n",
    "for filepath in glob.glob(blob_filepath):\n",
    "    print(filepath)\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line))\n",
    "print(len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', '科技')]\n"
     ]
    }
   ],
   "source": [
    "# regex_pattern = r\"\\[(\\d+)\\] ([\\u4e00-\\u9fff]+)\"\n",
    "\n",
    "# response = \"[1] 科技：提及互联网和在线活动。\"\n",
    "# parts = regex.compile(regex_pattern).findall(response)\n",
    "# print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. extract responses from Weibo dataset\n",
    "\n",
    "ch_to_en = {\n",
    "    \"科技\": \"Technology\",\n",
    "    \"体育\": \"Sports\",\n",
    "    \"社会问题\": \"Social Issues\",\n",
    "    \"历史\": \"History\",\n",
    "    \"娱乐\": \"Entertainment\",\n",
    "    \"农业\": \"Agriculture\",\n",
    "    \"政治\": \"Politics\",\n",
    "    \"军事\": \"Military\",\n",
    "    \"教育\": \"Education\",\n",
    "    \"经济\": \"Economy\",\n",
    "    \"环境\": \"Environment\",\n",
    "    \"贸易\": \"Trade\",\n",
    "    \"文化\": \"Culture\",\n",
    "    \"地缘政治\": \"Geopolitics\",\n",
    "    \"文学\": \"Literature\",\n",
    "    \"食品\": \"Food\",\n",
    "    \"天气\": \"Weather\",\n",
    "    \"媒体\": \"Media\",\n",
    "    \"社交媒体\": \"Social Media\",\n",
    "    \"外交\": \"Diplomacy\",\n",
    "    \"犯罪\": \"Crime\",\n",
    "    \"安全\": \"Security\",\n",
    "    \"健康\": \"Health\",\n",
    "    \"交通\": \"Transportation\",\n",
    "    \"政府\": \"Government\",\n",
    "    \"国际关系\": \"International Relations\",\n",
    "    \"法律\": \"Law\",\n",
    "    \"宗教\": \"Religion\",\n",
    "    \"旅游\": \"Tourism\",\n",
    "    \"食品安全\": \"Food Safety\",\n",
    "    \"商业\": \"Business\",\n",
    "    \"社交\": \"Social Interaction\"\n",
    "}\n",
    "\n",
    "u2idx = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/u2idx.data\")\n",
    "\n",
    "def generate_interest_cascades(data_dict: list, remove: bool = False):\n",
    "    # regex_pattern = r\"\\[(\\d+)\\] ([\\w\\s]+): ([\\w\\s]+)\\.\"\n",
    "    regex_pattern = r\"\\[(\\d+)\\] ([\\u4e00-\\u9fff]+)\"\n",
    "\n",
    "    interest_cascades = {}\n",
    "    for elem in data_dict:\n",
    "        if \"response\" in elem or \"responses\" in elem:\n",
    "            # response = elem[\"response\"]\n",
    "            response = elem[\"responses\"]\n",
    "            parts = regex.compile(regex_pattern).findall(response)\n",
    "            if len(parts) == 0: continue\n",
    "            for part in parts:\n",
    "                _, t = part[:2]\n",
    "                if t not in ch_to_en: continue\n",
    "                t = ch_to_en[t]\n",
    "                interest = gt_ft_mapping[t]\n",
    "                if interest not in interest_cascades: interest_cascades[interest] = []\n",
    "                mt_info = elem[\"meta_info\"]\n",
    "                if mt_info[\"user\"] not in u2idx: continue\n",
    "                interest_cascades[interest].append((u2idx[mt_info[\"user\"]], int(mt_info[\"ts\"]), elem[\"text\"]))\n",
    "        elif \"topicgpt_label\" in elem:\n",
    "            interest = elem[\"topicgpt_label\"]\n",
    "            # interest = gt_ft_mapping[t]\n",
    "            if interest not in interest_cascades: interest_cascades[interest] = []\n",
    "            # mt_info = elem[\"meta_info\"]\n",
    "            if elem[\"meta_info\"][\"user\"] not in u2idx: continue\n",
    "            interest_cascades[interest].append((u2idx[elem[\"meta_info\"][\"user\"]], int(elem[\"meta_info\"][\"ts\"]), elem[\"text\"]))\n",
    "        else: continue\n",
    "    print(len(interest_cascades))\n",
    "    \n",
    "    # # remove minor interests\n",
    "    # interest_cascades_cp = interest_cascades.copy()\n",
    "    # for t, cascades in interest_cascades_cp.items():\n",
    "    #     if len(cascades) < 10:\n",
    "    #         interest_cascades.pop(t)\n",
    "    # print(len(interest_cascades))\n",
    "\n",
    "    # # sort by timestamp\n",
    "    # for t in interest_cascades:\n",
    "    #     interest_cascades[t] = sorted(interest_cascades[t], key=lambda x: x[1])\n",
    "    return interest_cascades\n",
    "\n",
    "# interest_cascades = generate_interest_cascades(results, remove=True)\n",
    "# print(sum([len(v) for k, v in interest_cascades.items()]))\n",
    "\n",
    "# for h, cascades in interest_cascades.items():\n",
    "#     # print(h, len(cascades))\n",
    "#     for elem in cascades:\n",
    "#         print(elem)\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8404, 9968), (3149, 9067), (6100, 8201), (9584, 10050), (8787, 9706), (1126, 1126), (6905, 6908), (1676, 5730), (7899, 8520), (8057, 9860)]\n"
     ]
    }
   ],
   "source": [
    "# graph_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/topic_diffusion_graph_seed_user_windowsize7.data\"\n",
    "# graph_d = load_pickle(graph_filepath)\n",
    "\n",
    "# for key, g in graph_d.items():\n",
    "#     edges = g.edge_index.t().tolist()\n",
    "#     edges = [(e[0], e[1]) for e in edges]\n",
    "#     print(edges[:10])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/topic_diffusion_graph_ex_2000user_windowsize7.data\n",
      "4973\n",
      "114600\n",
      "23\n",
      "62663\n",
      "torch.Size([2, 922845])\n",
      "torch.Size([2, 1234719])\n",
      "torch.Size([2, 1188717])\n",
      "torch.Size([2, 876565])\n",
      "torch.Size([2, 989200])\n",
      "torch.Size([2, 1040004])\n",
      "torch.Size([2, 904217])\n",
      "torch.Size([2, 863642])\n",
      "torch.Size([2, 853896])\n",
      "torch.Size([2, 840969])\n",
      "torch.Size([2, 814099])\n",
      "torch.Size([2, 838895])\n",
      "torch.Size([2, 605019])\n",
      "torch.Size([2, 601959])\n",
      "torch.Size([2, 511911])\n",
      "torch.Size([2, 581439])\n",
      "torch.Size([2, 487040])\n",
      "torch.Size([2, 325276])\n",
      "torch.Size([2, 313699])\n",
      "torch.Size([2, 310500])\n"
     ]
    }
   ],
   "source": [
    "# 2. generate simedges from interest-aware cascades\n",
    "\n",
    "# def generate_interest_cascades(data_dict: list, remove: bool = False):\n",
    "#     regex_pattern = r\"\\[(\\d+)\\] ([\\w\\s]+): ([\\w\\s]+)\\.\"\n",
    "\n",
    "#     interest_cascades = {}\n",
    "#     for elem in data_dict:\n",
    "#         if \"response\" in elem:\n",
    "#             response = elem[\"responses\"]\n",
    "#             parts = regex.compile(regex_pattern).findall(response)\n",
    "#             if len(parts) > 0: parts = parts[0]\n",
    "#             else: continue\n",
    "#             _, t, _ = parts[:3]\n",
    "#             interest = gt_ft_mapping[t]\n",
    "#             if interest not in interest_cascades: interest_cascades[interest] = []\n",
    "#             mt_info = elem[\"meta_info\"]\n",
    "#             interest_cascades[interest].append((mt_info[\"user\"], int(mt_info[\"ts\"]), elem[\"text\"]))\n",
    "#         elif \"topicgpt_label\" in elem:\n",
    "#             interest = elem[\"topicgpt_label\"]\n",
    "#             # interest = gt_ft_mapping[t]\n",
    "#             if interest not in interest_cascades: interest_cascades[interest] = []\n",
    "#             # mt_info = elem[\"meta_info\"]\n",
    "#             interest_cascades[interest].append((elem[\"user\"], int(elem[\"ts\"]), elem[\"text\"]))\n",
    "#         else: continue\n",
    "#     print(len(interest_cascades))\n",
    "    \n",
    "#     # remove minor interests\n",
    "#     interest_cascades_cp = interest_cascades.copy()\n",
    "#     for t, cascades in interest_cascades_cp.items():\n",
    "#         if len(cascades) < 10:\n",
    "#             interest_cascades.pop(t)\n",
    "#     print(len(interest_cascades))\n",
    "\n",
    "#     # sort by timestamp\n",
    "#     for t in interest_cascades:\n",
    "#         interest_cascades[t] = sorted(interest_cascades[t], key=lambda x: x[1])\n",
    "#     return interest_cascades\n",
    "\n",
    "def generate_simedges(interest_cascades: dict, time_distance: int = 3600 * 24 * 30):\n",
    "    simedges = {}\n",
    "    for interest, cascades in interest_cascades.items():\n",
    "        simedges[interest] = []\n",
    "        for i in range(len(cascades)-1):\n",
    "            for j in range(i, len(cascades)-1):\n",
    "                if cascades[j][1] - cascades[i][1] < time_distance and \\\n",
    "                    cascades[j][0] != cascades[i][0]:\n",
    "                # if cascades[j][0] != cascades[i][0]:\n",
    "                    simedges[interest].append((cascades[i][0], cascades[j][0]))\n",
    "        # for i in range(len(cascades)-1):\n",
    "        #     for j in range(max(0,i-1-window_size),min(i+1+window_size,len(cascades))): # (i-ws-1<-i->i+ws+1)\n",
    "        #         if cascades[i][0] != cascades[j][0]:\n",
    "        #             simedges[interest].append((cascades[i][0],cascades[j][0]))\n",
    "    \n",
    "    # remove abundant edges\n",
    "    for interest, edges in simedges.items():\n",
    "        simedges[interest] = list(set(edges))\n",
    "    return simedges\n",
    "\n",
    "# 3. convert simedges to graph·\n",
    "def convert_to_graph(simedges: dict, user_size: int, originial_edges: Optional[list] = None, original_simedges: Optional[dict] = None):\n",
    "    if originial_edges:\n",
    "        for interest, edges in simedges.items():\n",
    "            simedges[interest] = list(set(edges + originial_edges))\n",
    "    \n",
    "    if original_simedges:\n",
    "        all_interests = list(set(simedges.keys()) | set(original_simedges.keys()))\n",
    "        print(simedges.keys())\n",
    "        print(original_simedges.keys())\n",
    "        print(all_interests)\n",
    "        for interest in all_interests:\n",
    "            if interest not in simedges: simedges[interest] = original_simedges[interest]\n",
    "            elif interest not in original_simedges: simedges[interest] = simedges[interest]\n",
    "            else: simedges[interest] = list(set(simedges[interest] + original_simedges[interest]))\n",
    "    \n",
    "    # add self-loop edges\n",
    "    for interest, edges in simedges.items():\n",
    "        edges += [(u,u) for u in range(user_size)]\n",
    "        simedges[interest] = list(set(edges))\n",
    "    \n",
    "    # convert to graph\n",
    "    graph_d = {}\n",
    "    for interest, edges in simedges.items():\n",
    "        edges = list(zip(*edges))\n",
    "        edges_t = torch.LongTensor(edges)\n",
    "        weight_t = torch.FloatTensor([1]*edges_t.size(1))\n",
    "        graph_d[interest] = Data(edge_index=edges_t, edge_weight=weight_t)\n",
    "    \n",
    "    return graph_d\n",
    "\n",
    "# dataset = 'Twitter'\n",
    "# save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/topic_diffusion_graph_{name}_windowsize{td}.data\"\n",
    "# u2idx = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/u2idx.data\")\n",
    "# original_edges = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/edges.data\")\n",
    "# previous_mp = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/topic_llm2/topic_diffusion_graph_seed_user_windowsize7.data\")\n",
    "\n",
    "# dataset = 'Weibo'\n",
    "save_filepath = \"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/topic_diffusion_graph_{name}_windowsize{td}.data\"\n",
    "u2idx = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/u2idx.data\")\n",
    "original_edges = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/edges.data\")\n",
    "previous_mp = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Weibo-Aminer/topic_llm2/topic_diffusion_graph_seed_user_windowsize7.data\")\n",
    "\n",
    "days = 7\n",
    "filename = \"ex_2000user\" # seed_user\n",
    "time_distance = 3600 * 24 * days\n",
    "save_filepath = save_filepath.format(name=filename, td=days)\n",
    "print(save_filepath)\n",
    "print(len(u2idx))\n",
    "print(len(original_edges))\n",
    "\n",
    "interest_cascades = generate_interest_cascades(results, remove=True)\n",
    "print(sum([len(v) for k, v in interest_cascades.items()]))\n",
    "\n",
    "simedges = generate_simedges(interest_cascades, time_distance)\n",
    "# select top20 interests\n",
    "len_mp = {k:len(v) for k,v in simedges.items()}\n",
    "for k in list(sort_dict(len_mp).keys())[20:]:\n",
    "    # print(k, len(simedges[k]))\n",
    "    simedges.pop(k)\n",
    "\n",
    "original_simedges = {}\n",
    "for k, graph in previous_mp.items():\n",
    "    edges_t = graph.edge_index\n",
    "    edges = list(zip(edges_t[0].tolist(), edges_t[1].tolist()))\n",
    "    original_simedges[k] = edges\n",
    "\n",
    "graph_d = convert_to_graph(simedges, user_size=len(u2idx), originial_edges=original_edges)\n",
    "for key, graph in graph_d.items():\n",
    "    print(graph.edge_index.size())\n",
    "save_pickle(graph_d, save_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = load_pickle(\"/remote-home/share/dmb_nas/wangzejian/HeterGAT/Twitter-Huangxin/sub10000/train.data\")\n",
    "\n",
    "# dist = []\n",
    "# for tag, cascades in train_data.items():\n",
    "#     ts_list = cascades['ts']\n",
    "#     if len(ts_list) < 2 or len(ts_list) > 500: continue\n",
    "#     # if ts_list[-1] - ts_list[0] < 24 * 3600:\n",
    "#     #     print(tag)\n",
    "#     dist.append(np.ceil((ts_list[-1] - ts_list[0]) / (3600 * 24)))\n",
    "\n",
    "# for i in range(10):\n",
    "#     t = np.percentile(dist, i*10)\n",
    "#     print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "\n",
    "# # inspect time distance distribution\n",
    "# time_diff = 3600\n",
    "# time_distance = []\n",
    "# for interest, cascades in interest_cascades.items():\n",
    "#     for elem in cascades:\n",
    "#         time_distance.append(elem[1] / time_diff)\n",
    "#         # print(elem[1])\n",
    "#         # print(datetime.datetime.utcfromtimestamp(elem[1]))\n",
    "#         # print(datetime.datetime.utcfromtimestamp(elem[1] - time_diff))\n",
    "#     break\n",
    "\n",
    "# def analyse_distribution(data):\n",
    "#     for i in range(10):\n",
    "#         t = np.percentile(data, i*10)\n",
    "#         print(datetime.datetime.utcfromtimestamp(t * time_diff))\n",
    "\n",
    "# analyse_distribution(time_distance)\n",
    "# # simedges = generate_simedges(generate_interest_cascades(results, remove=True), window_size)\n",
    "# # select top20 interests\n",
    "# # len_mp = {k:len(v) for k,v in simedges.items()}\n",
    "# # for k in list(sort_dict(len_mp).keys())[20:]:\n",
    "# #     print(k, len(simedges[k]))\n",
    "# #     simedges.pop(k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
